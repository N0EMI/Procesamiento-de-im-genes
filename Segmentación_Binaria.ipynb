{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 199679295,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/N0EMI/Procesamiento-de-im-genes/blob/master/Segmentaci%C3%B3n_Binaria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Descargar el conjunto de datos"
      ],
      "metadata": {
        "id": "nqPrlzogJWsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "ivarvb_ds2024iidccecv2_path = kagglehub.dataset_download('ivarvb/ds2024iidccecv2')"
      ],
      "metadata": {
        "id": "FOEFmLxhHlV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "ivarvb_ds2024iidccecv2_path = kagglehub.dataset_download('ivarvb/ds2024iidccecv2')"
      ],
      "metadata": {
        "id": "YWI9bhQpMgOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Segmentación binaria de imágenes\n",
        "\n",
        "TODO: Describe que es la segmentación binaria de imágenes"
      ],
      "metadata": {
        "id": "Tya-W8-kJfhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Importando bibliotecas"
      ],
      "metadata": {
        "id": "ocLpOPSEJrhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.layers import SpatialDropout2D,Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate,AveragePooling2D, UpSampling2D, BatchNormalization, Activation, add,Dropout,Permute,ZeroPadding2D,Add, Reshape\n",
        "\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Accuracy, Recall, Precision\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "from tensorflow.python.keras.backend import set_session\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "PVPb70nJJ_q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Asegurando Reproductibilidad"
      ],
      "metadata": {
        "id": "n9QzzV2HKHr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setseed():\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    #os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "    SEED = 1030\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "    #*********************************************************************************#\n",
        "    # fix seed for reproducible results (only works on CPU, not GPU)\n",
        "    seed_value = SEED #default\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "    #tf.set_random_seed(seed_value)\n",
        "    #config = tf.ConfigProto()\n",
        "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    config.gpu_options.allow_growth = True\n",
        "    set_session(tf.compat.v1.Session(graph = tf.compat.v1.get_default_graph(), config = config))\n",
        "\n",
        "setseed()\n",
        "\n",
        "print(\"tf.version\",tf.__version__)\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "\n",
        "inputdir = \"/kaggle/input/ds2024iidccecv2/melanoma/\"\n",
        "outputdir = \"/kaggle\"\n",
        "\n",
        "pathtrain = os.path.join(inputdir,\"train\")\n",
        "pathtest = os.path.join(inputdir,\"test\")\n",
        "pathtrainaug = os.path.join(inputdir, \"train_augmented\")\n",
        "\n",
        "csv_path = os.path.join(outputdir, \"Epoch_Log.csv\")\n",
        "\n",
        "model_weights_path = os.path.join(outputdir, \"model_weights.weights.h5\")\n",
        "model_path = os.path.join(outputdir, \"model.keras\")\n",
        "\n",
        "\n",
        "# LR = 2e-4\n",
        "LR = 2e-5\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "batches=1\n",
        "#epochs=80\n",
        "epochs=30\n",
        "\n",
        "stop_early=False\n",
        "SMOOTH = 1e-15\n",
        "\n",
        "# LR = 1e-5\n",
        "# LR = 4e-4\n",
        "\n",
        "# MEAN = [0.485, 0.456, 0.406]\n",
        "# STD = [0.229, 0.224, 0.225]"
      ],
      "metadata": {
        "id": "hEf_ZHrXKWWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Implementado el modelo\n",
        "\n",
        "TODO: Explica el modelo a usarse (UNet), puedes apoyarte de: https://towardsdatascience.com/image-segmentation-unet-and-deep-supervision-loss-using-keras-model-f21a9856750a"
      ],
      "metadata": {
        "id": "dWHQft3MKc4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ref from: https://towardsdatascience.com/image-segmentation-unet-and-deep-supervision-loss-using-keras-model-f21a9856750a\n",
        "\"\"\"\n",
        "def conv_block(inp, filters, padding='same', activation='relu'):\n",
        "    x = Conv2D(filters, (3, 3), padding=padding, activation=activation)(inp)\n",
        "    x = Conv2D(filters, (3, 3), padding=padding)(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def encoder_block(inp, filters, padding='same', pool_stride=2, activation='relu'):\n",
        "    x = conv_block(inp, filters, padding, activation)\n",
        "    p = MaxPooling2D(pool_size=(2, 2), strides=pool_stride)(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inp,filters,concat_layer,padding='same'):\n",
        "    #Upsample the feature maps\n",
        "    x=Conv2DTranspose(filters,(2,2),strides=(2,2),padding=padding)(inp)\n",
        "    x=concatenate([x,concat_layer])#Concatenation/Skip conncetion with conjuagte encoder\n",
        "    x=conv_block(x,filters)#Passed into the convolution block above\n",
        "    return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "    \"\"\" Input \"\"\"\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    d1,p1=encoder_block(inputs,64)\n",
        "    d2,p2=encoder_block(p1,128)\n",
        "    d3,p3=encoder_block(p2,256)\n",
        "    d4,p4=encoder_block(p3,512)\n",
        "\n",
        "    mid=conv_block(p4,1024) #Midsection\n",
        "\n",
        "    e2=decoder_block(mid,512,d4) #Conjugate of encoder 4\n",
        "    e3=decoder_block(e2,256,d3) #Conjugate of encoder 3\n",
        "    e4=decoder_block(e3,128,d2) #Conjugate of encoder 2\n",
        "    e5=decoder_block(e4,64,d1) #Conjugate of encoder 1\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(e5) #Final Output\n",
        "    model=Model(inputs, outputs, name='Unet')\n",
        "#     model=Model(inputs=[inputs],outputs=[outputs,o1],name='Unet')\n",
        "#     model = Model(inputs, outputs, name=\"ResNet50_U-Net\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "FjzMmFjrKlJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Métricas\n",
        "\n",
        "TODO: Describe las métricas IoU, Coeficiente Dice, accuracy, precision, recall y f1-score. Deberás incluir fórmulas en formato markdown e imágenes cuando sea necesario."
      ],
      "metadata": {
        "id": "po2bmrnKKmsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = K.flatten(y_true)\n",
        "    y_pred = K.flatten(y_pred)\n",
        "\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    x = (intersection + SMOOTH) / (union + SMOOTH)\n",
        "    return x\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = K.flatten(y_true)\n",
        "    y_pred = K.flatten(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + SMOOTH) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + SMOOTH)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "# Evaluation Metrics\n",
        "def eval_iou(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true[y_true > 0] = 1  # normalize binary mask\n",
        "    y_true = y_true.astype(np.float32)\n",
        "\n",
        "    intersection = (y_true * y_pred).sum()\n",
        "    union = y_true.sum() + y_pred.sum() - intersection\n",
        "\n",
        "    x = (intersection + SMOOTH) / (union + SMOOTH)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "\n",
        "def eval_dice_coef(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true[y_true > 0] = 1  # normalize binary mask\n",
        "    y_true = y_true.astype(np.float32)\n",
        "\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    union = np.sum(y_true) + np.sum(y_pred)\n",
        "\n",
        "    if union == 0:\n",
        "        return 1.0  # Handle the case where both masks are empty\n",
        "\n",
        "    dice = (2.0 * intersection + SMOOTH) / (union + SMOOTH)\n",
        "    return dice\n",
        "\n",
        "\n",
        "def accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true[y_true > 0] = 1  # normalize binary mask\n",
        "    y_true = y_true.astype(np.float32)\n",
        "\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = y_true.size\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def precision_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true[y_true > 0] = 1  # normalize binary mask\n",
        "    y_true = y_true.astype(np.float32)\n",
        "\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "\n",
        "    if TP + FP == 0:\n",
        "        return 1.0  # case where both TP and FP are zero\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true[y_true > 0] = 1  # normalize binary mask\n",
        "    y_true = y_true.astype(np.float32)\n",
        "\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    if TP + FN == 0:\n",
        "        return 1.0  # case where TP and FN are zero\n",
        "\n",
        "    recall = TP / (TP + FN)\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true[y_true > 0] = 1  # normalize binary mask\n",
        "    y_true = y_true.astype(np.float32)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0.0  # Handle the case where both precision and recall are zero\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "metadata": {
        "id": "9Rdsj4slK2rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Cargando los datos"
      ],
      "metadata": {
        "id": "WcFyvth8K877"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# H = 256\n",
        "# W = 256\n",
        "\n",
        "\n",
        "# def shuffling(x: list[str], y: list[str]) -> list[str]:\n",
        "#     x, y = shuffle(x, y, random_state=42)\n",
        "#     return x, y\n",
        "\n",
        "def loadDataAug(path: str) -> list[str]:\n",
        "    x = sorted(glob(os.path.join(path, \"images\", \"*jpg\")))\n",
        "    y = sorted(glob(os.path.join(path, \"groundtruths\", \"*jpg\")))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def loadData(path: str) -> list[str]:\n",
        "    x = sorted(glob(os.path.join(path, \"images\", \"*jpg\")))\n",
        "    y = sorted(glob(os.path.join(path, \"groundtruths\", \"*jpg\")))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def read_image_t(path: Any) -> Any:\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "\n",
        "#     x = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32)\n",
        "    x = cv2.resize(x, (H, W))\n",
        "\n",
        "    x = x / 255.0  # normalizing and standardizing image with Imagenet specifications\n",
        "#     x -= MEAN\n",
        "#     x /= STD\n",
        "    return x\n",
        "\n",
        "\n",
        "def read_image(path: Any) -> Any:\n",
        "    path = path.decode()\n",
        "    return read_image_t(path)\n",
        "\n",
        "\n",
        "def read_mask_t(path: Any) -> Any:\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "    x = cv2.resize(x, (H, W))\n",
        "\n",
        "    x = x / 255.0  # normalizing mask\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x\n",
        "\n",
        "def read_mask(path: Any) -> Any:\n",
        "    path = path.decode()\n",
        "    return read_mask_t(path)\n",
        "\n",
        "\n",
        "def tf_parse(x: Any, y: Any) -> Any:\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "#     print(\"x.shape, y.shape\", x.shape, y.shape)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def tf_dataset(X: Any, Y: Any, batch: int = 2) -> Any:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def readDatasetFiles(ptrain, ptest, val_size=0.1, ptrainaugment=None):\n",
        "    x_test, y_test = loadData(ptest)\n",
        "    x_train, y_train = loadData(ptrain)\n",
        "    # print(\"XX\",x_test, y_test, x_train, y_train)\n",
        "\n",
        "#     if not ptrainaugment == None:\n",
        "#         x_train_aug, y_train_aug = loadDataAug(ptrainaugment)\n",
        "#         print(f\"Train_aug:\\n Images: {len(x_train)}\\tMasks: {len(y_train)}\\n\")\n",
        "#         x_train += x_train_aug\n",
        "#         y_train += y_train_aug\n",
        "\n",
        "    x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
        "#     x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=len(x_test))\n",
        "\n",
        "    print(f\"Test:\\n Images: {len(x_test)}\\tMasks: {len(y_test)}\\n\")\n",
        "    print(f\"Train:\\n Images: {len(x_train)}\\tMasks: {len(y_train)}\\n\")\n",
        "    print(f\"Validation:\\n Images: {len(x_val)}\\tMasks: {len(y_val)}\\n\")\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "metadata": {
        "id": "gVudM3dTK8oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Funciones de evaluación"
      ],
      "metadata": {
        "id": "2yfFIbzbLFpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "#### BEGING: EVALUATE ##########\n",
        "################################\n",
        "def vis(filename, image, mask, predicted):\n",
        "    namef = os.path.splitext(os.path.basename(filename))[0]\n",
        "\n",
        "    # results - plot\n",
        "    fig, ax = plt.subplots(1,3, figsize=(10,15))\n",
        "    ax[0].imshow(image, 'gray')\n",
        "    ax[0].set_title('image')\n",
        "\n",
        "    ax[1].imshow(mask, 'gray')\n",
        "    ax[1].set_title('GT')\n",
        "\n",
        "#     ax[2].imshow(predicted, cmap='jet')\n",
        "    ax[2].imshow(predicted, cmap='gnuplot')\n",
        "    ax[2].set_title('Predicted')\n",
        "\n",
        "def savenp(filen, obj):\n",
        "    with open(filen, 'wb') as f:\n",
        "        np.save(f, obj)\n",
        "\n",
        "def makedir(dirpath):\n",
        "    if not os.path.isdir(dirpath):\n",
        "        os.mkdir(dirpath)\n",
        "\n",
        "def evaluateModel(model, testds, batches):\n",
        "    test_x, test_y = testds\n",
        "\n",
        "    model.load_weights(model_weights_path)\n",
        "    model.save(model_path)\n",
        "    data = {'dice': [], 'iou': []}\n",
        "    for i in range(len(test_y)):\n",
        "        namef = os.path.splitext(os.path.basename(test_x[i]))[0]\n",
        "        image = read_image_t(test_x[i])\n",
        "\n",
        "        y_r = read_mask_t(test_y[i]) #real\n",
        "        y_p = model.predict(tf.expand_dims(image,axis=0))[0]#predicted\n",
        "\n",
        "        savenp(os.path.join(outputdir, \"results\", namef+\".npy\"), y_p)\n",
        "\n",
        "        y_p = (y_p > 0.5).astype(np.float32)\n",
        "\n",
        "        print(\"y_p.shape, y_r.shape\", y_p.shape, y_r.shape)\n",
        "\n",
        "        data[\"dice\"].append(round(dice_coef(y_r, y_p).numpy(), 4))\n",
        "        data[\"iou\"].append(round(iou(y_p, y_r).numpy(), 4))\n",
        "\n",
        "        vis(test_x[i], image, y_r, y_p)\n",
        "\n",
        "    pdf = pd.DataFrame.from_dict(data)\n",
        "    pdf.to_csv(os.path.join(outputdir, \"test_metrics.csv\"))\n",
        "    print(\"pdf\", pdf)\n",
        "\n",
        "    print('mean: ', pdf.mean(axis=0))\n",
        "    print('std: ', pdf.std(axis=0))\n",
        "\n",
        "################################\n",
        "#### END: EVALUATE ##########\n",
        "################################\n"
      ],
      "metadata": {
        "id": "IsInck2rLHwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "FPBEa-0qLMKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    makedir(os.path.join(outputdir, \"results\"))\n",
        "\n",
        "    # read files from path\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = readDatasetFiles(pathtrain,\n",
        "                                                                      pathtest,\n",
        "                                                                      val_size=0.1,\n",
        "                                                                      ptrainaugment=pathtrainaug\n",
        "                                                                     )\n",
        "\n",
        "\n",
        "    # read images from files to tf\n",
        "    train_ds = tf_dataset(x_train, y_train, batch=batches)\n",
        "    val_ds = tf_dataset(x_val, y_val, batch=batches)\n",
        "    #test_ds = tf_dataset(x_test, y_test, batch=batches)\n",
        "\n",
        "    model = build_unet((H, W, 3))\n",
        "\n",
        "\n",
        "    # compile model\n",
        "    model.compile(\n",
        "        loss=dice_loss,\n",
        "        optimizer=Adam(LR),\n",
        "#         optimizer='adam',\n",
        "        metrics=[dice_coef, iou],\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_weights_path, verbose=1, save_best_only=True, save_weights_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(log_dir=\"logs\"),\n",
        "\n",
        "    ]\n",
        "\n",
        "    if stop_early:\n",
        "        callbacks.append(\n",
        "            EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=False),\n",
        "        )\n",
        "\n",
        "    # training mondel\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        batch_size=batches,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # test evaluation\n",
        "    evaluateModel(model, (x_test, y_test), batches)"
      ],
      "metadata": {
        "id": "vjE7p9qKIFeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}